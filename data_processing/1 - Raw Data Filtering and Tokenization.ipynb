{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Raw Data Filtering and Tokenization\n",
    "Given raw social media data, tokenize and filter the data to prepare it for annotation by Amazon Mechanical Turkers, or direct prediction from the model. For processing data that has been annotated, see \"Annotated Data Filtering and Tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from custom_tokenizer import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of all possible data\n",
    "filepath = '../data/third_data'\n",
    "df = pd.read_json(filepath+'.json')\n",
    "\n",
    "# special-case for third_data.json\n",
    "df = df.rename(columns={\"text\": \"question\", \"pid\": \"post_id\"})\n",
    "\n",
    "# ensure no index overlap\n",
    "index_offset = len(pd.read_json('../data/original_data.json'))\n",
    "index_offset += len(pd.read_json('../data/second_data.json'))\n",
    "\n",
    "df.index += index_offset\n",
    "\n",
    "df['source_file'] = filepath\n",
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/answers_vqa.txt\") as f:\n",
    "    valid_ans = set()\n",
    "    for row in f:\n",
    "        valid_ans.add(str.strip(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['r_tokenization'] = df.response.apply(lambda x: response_tokenize(x))\n",
    "df['q_tokenization'] = df.question.apply(lambda x: question_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove emojis and non-ASCII characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import emoji\n",
    "emoji_regex = emoji.get_emoji_regexp()\n",
    "def filter_unicode(x):\n",
    "    filtered_tokens = []\n",
    "    for token in x:\n",
    "        if token == '': continue\n",
    "        # skip anything that isn't a letter\n",
    "        if len(token) == 1 and unicodedata.category(token)[0] != 'L':\n",
    "            continue\n",
    "        else:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "df['r_tokenization'] = df.r_tokenization.apply(lambda x: [emoji_regex.sub(r'', token) for token in x])\n",
    "df['r_tokenization'] = df.r_tokenization.apply(lambda x: filter_unicode(x))\n",
    "df['response_filtered'] = df.r_tokenization.apply(lambda x: \" \".join(x))\n",
    "df['response_invalid'] = df.response_filtered.apply(lambda x: not x.isascii())\n",
    "response_invalid = df[df.response_invalid == True]\n",
    "print(\"Now dropping {} rows where unicode characters were still present...\".format(len(response_invalid)))\n",
    "print(\"Examples: \", \"; \".join(response_invalid.head(5).response_filtered.values))\n",
    "df = df.drop(response_invalid.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove certain questions known to cause confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_questions = df[df.q_tokenization.str[0] == \"where\"]\n",
    "print(\"Now dropping {} rows of bad questions...\".format(len(bad_questions)))\n",
    "print(\"Examples: \", \" \".join(bad_questions.head(5).question.values))\n",
    "df = df.drop(bad_questions.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict to responses that could contain VQA 2.0 vocab only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yes_no(response):\n",
    "    if response is None: return\n",
    "    for idx, token in enumerate(response):\n",
    "        if token in ['yep', 'yup', 'yeah', 'yess', 'yesss']:\n",
    "            response[idx] = 'yes'\n",
    "        elif token in ['nope']:\n",
    "            response[idx] = 'no'\n",
    "    return response\n",
    "\n",
    "df.r_tokenization = df.r_tokenization.apply(lambda x: convert_yes_no(x))\n",
    "\n",
    "def vocab_in_response(response):\n",
    "    for token in response:\n",
    "        if token in valid_ans: return True\n",
    "    return False\n",
    "df['in_vocab'] = df.r_tokenization.apply(lambda x: vocab_in_response(x))\n",
    "out_of_vocab = df[df.in_vocab == False]\n",
    "print(\"Now dropping {} rows of responses that don't have any in-vocab tokens...\".format(len(out_of_vocab)))\n",
    "print(\"Examples: \", \"; \".join(out_of_vocab.head(25).response_filtered.values))\n",
    "df = df.drop(out_of_vocab.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview and Save Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(filepath+'_filtered.csv', index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
