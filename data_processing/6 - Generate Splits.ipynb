{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Generate Splits\n",
    "Given the filtered data generated by \"Annotated Data Filtering and Tokenization\", split the data into train, val, test for predicting question relevance and exracting answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from custom_tokenizer import find_start_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlabelled data\n",
    "first_df = pd.read_csv('../data/original_data_filtered.csv', index_col='index')\n",
    "second_df = pd.read_csv('../data/second_data_filtered.csv', index_col='index')\n",
    "third_df = pd.read_csv('../data/third_data_filtered.csv', index_col='index')\n",
    "\n",
    "unlabelled_df = pd.concat([first_df, second_df, third_df], sort=True)\n",
    "\n",
    "# labelled data (subset of the above)\n",
    "first_df = pd.read_csv('../data/initial_MTurk_test_filled.csv', index_col='index')\n",
    "second_df = pd.read_csv('../data/second_MTurk_test_filled.csv', index_col='index')\n",
    "\n",
    "labelled_df = pd.concat([first_df, second_df], sort=True)\n",
    "print(len(labelled_df))\n",
    "unlabelled_df = unlabelled_df.drop(labelled_df.index)\n",
    "print(len(unlabelled_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out unlabelled data to be cleaned by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_df = unlabelled_df[unlabelled_df.img_found == True]\n",
    "dups = unlabelled_df.duplicated(subset=['post_id'], keep=False)\n",
    "print(\"{} rows were duplicates...dropping...\".format(len(dups[dups == True])))\n",
    "duplicated_rows = unlabelled_df.loc[dups[dups == True].index]\n",
    "unlabelled_df = unlabelled_df.drop(duplicated_rows.index)\n",
    "\n",
    "unlabelled_df.to_csv('../data/custom_dataset_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out labeled data to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv('../data/gold_dev.csv', index_col='index')\n",
    "val_df = pd.read_csv('../data/r_relevance_val.csv', index_col='index')\n",
    "df = val_df.join(dev_df, rsuffix='2')\n",
    "df.q_relevant = df.apply(lambda row: row.gold_q_relevant if pd.notnull(row.gold_q_relevant) else row.q_relevant, axis=1)\n",
    "df.r_relevant = df.apply(lambda row: row.gold_r_relevant if pd.notnull(row.gold_r_relevant) else row.r_relevant, axis=1)\n",
    "df.answer_intersection_span = df.apply(lambda row: row.gold_answer_span if pd.notnull(row.gold_answer_span) else row.answer_intersection_span, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_sample(df, percentage, groupby_cols, seed=2019):\n",
    "        # how many rows should be selected\n",
    "        total_num = len(df)*percentage\n",
    "\n",
    "        # how many groups there will be (i.e. how many possible combos of each col values)\n",
    "        num_samples_taken = np.prod([len(df[col].unique()) for col in groupby_cols])\n",
    "\n",
    "        # how many rows should be in each group\n",
    "        num_per_sample = int(total_num/num_samples_taken)\n",
    "\n",
    "        # sample the groups\n",
    "        if groupby_cols:\n",
    "            chosen = df.groupby(groupby_cols).apply(lambda x: x.sample(num_per_sample, random_state=seed)).index.levels[len(groupby_cols)]\n",
    "        else:\n",
    "            chosen = df.apply(lambda x: x.sample(num_per_sample, random_state=seed)).index\n",
    "\n",
    "        return df.loc[chosen]\n",
    "\n",
    "def generate_splits(df, groupby_cols, seed=2019, sizes=[0.1, 0.25, 0.5, 0.75]):\n",
    "    splits = []\n",
    "    \n",
    "    df_remaining = df.copy()\n",
    "\n",
    "    test = choose_sample(df_remaining, 0.1, groupby_cols, seed)\n",
    "    df_remaining = df_remaining.drop(test.index)\n",
    "    test.name = 'test'\n",
    "    splits.append(test)\n",
    "\n",
    "    val = choose_sample(df_remaining, 0.1, groupby_cols, seed)\n",
    "    df_remaining = df_remaining.drop(val.index)\n",
    "    val.name = 'val'\n",
    "    splits.append(val)\n",
    "    \n",
    "    minival = choose_sample(val, 0.01, groupby_cols, seed)\n",
    "    minival.name = 'minival'\n",
    "    splits.append(minival)\n",
    "    \n",
    "    train = df_remaining\n",
    "    train = train.append(duplicated_rows)\n",
    "    train.name = 'train'\n",
    "    splits.append(train)\n",
    "    \n",
    "    for size in sizes:\n",
    "        train_subset = choose_sample(train, size, [], seed)\n",
    "        train_subset.name = 'train_'+str(size)\n",
    "        splits.append(train_subset)\n",
    "        \n",
    "    minitrain = choose_sample(train, 0.01, groupby_cols, seed)\n",
    "    minitrain.name = 'minitrain'\n",
    "    splits.append(minitrain)\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dev = df[pd.notnull(df.gold_q_relevant)]\n",
    "gold_dev.to_csv('../data/gold_dev.csv', index_label='index')\n",
    "df = df.drop(gold_dev.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for predicting question relevance (50/50 relevant/irrelevant questions, no responses)\n",
    "cols = ['q_relevant', 'question', 'response_filtered']\n",
    "for split in generate_splits(df, ['q_relevant']):\n",
    "    split.to_csv('../data/q_relevance_'+split.name+'.csv', index_label='index', columns=cols)\n",
    "\n",
    "# dataset for extracting answers (50/50 relevant/irrelevant responses, where all questions are relevant)\n",
    "cols = ['r_relevant', 'question', 'response_filtered', 'answer_intersection_span']\n",
    "for split in generate_splits(df[df.q_relevant == True], ['r_relevant']):\n",
    "    split.to_csv('../data/r_relevance_'+split.name+'.csv', index_label='index', columns=cols)\n",
    "\n",
    "# dataset for both (TT, TF, FT, FF all even)\n",
    "cols = ['q_relevant', 'r_relevant', 'question', 'response_filtered', 'answer_intersection_span']\n",
    "for split in generate_splits(df, ['q_relevant', 'r_relevant']):\n",
    "    split.to_csv('../data/q+r_relevance_'+split.name+'.csv', index_label='index', columns=cols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
