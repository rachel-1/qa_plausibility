{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Annotated Data Filtering and Tokenization\n",
    "Given data annotated by the Amazon Mechanical Turk workers, format the answers properly. The data can then be split into train/val/test by \"Generate Splits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from custom_tokenizer import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Filled-In CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/second_MTurk_test_filled.csv', index_col='index')\n",
    "# convert tokenizations from strings to true lists\n",
    "df['q_tokenization'] = df.q_tokenization.apply(lambda x : pd.eval(x))\n",
    "df['r_tokenization'] = df.r_tokenization.apply(lambda x : pd.eval(x))\n",
    "df['span'] = df.span.apply(lambda x : pd.eval(x) if not pd.isnull(x) else None) # original labelling (before this project)\n",
    "df['turker_answer_span'] = df.turker_answer_span.apply(lambda x : pd.eval(x) if not pd.isnull(x) else None) # new MTurk label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract answer spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['span'] = df.apply(lambda row: find_start_end(row['r_tokenization'], row['answer']), axis=1)\n",
    "df['a_tokenization'] = df.apply(lambda row: extract_answer(row['r_tokenization'], row['span']), axis=1)\n",
    "df['turker_answer_span'] = df.apply(lambda row: find_start_end(row['r_tokenization'], row['turker_answer']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Intersection Between Extracted Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_overlap(first_answer_span, second_answer_span):\n",
    "    if second_answer_span is None: return None\n",
    "    if first_answer_span is None: return second_answer_span\n",
    "    first_tokens = set(range(first_answer_span[0], first_answer_span[1]+1))\n",
    "    second_tokens = set(range(second_answer_span[0], second_answer_span[1]+1))\n",
    "    intersection = first_tokens.intersection(second_tokens)\n",
    "    if len(intersection) == 0: return None\n",
    "    return (min(intersection), max(intersection))\n",
    "\n",
    "df['answer_intersection_span'] = df.apply(lambda row: answer_overlap(row.span, row.turker_answer_span), axis=1)\n",
    "\n",
    "def extract_ans(row):\n",
    "    answer_span = row.answer_intersection_span\n",
    "    if answer_span is None: return None\n",
    "    return row.r_tokenization[answer_span[0]:answer_span[1]+1]\n",
    "\n",
    "df['answer_intersection'] = df.apply(lambda row: extract_ans(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{} rows don't have any intersection\".format(len(df[(df.r_relevant) & (pd.isna(df.answer_intersection))])))\n",
    "# overwrite response relevance in cases where the answers don't overlap\n",
    "df.loc[(df.r_relevant) & (pd.isna(df.answer_intersection)), 'r_relevant'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract In-Vocab Phrases From Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_in_answer(original_answer):\n",
    "    if original_answer is None: return None\n",
    "    # find longest substring that is in the vocab\n",
    "    # start with ngram where n is the length of the string, then try each from there\n",
    "    for n in range(len(original_answer), 0, -1):\n",
    "        ngrams = [original_answer[i:i+n] for i in range(len(original_answer)-n+1)]\n",
    "        for ngram in ngrams[::-1]:\n",
    "            answer = \" \".join(ngram)\n",
    "            if answer in valid_ans: return answer\n",
    "    return None\n",
    "df['in_vocab_answer'] = df.apply(lambda row: vocab_in_answer(row.answer_intersection), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Out File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/second_MTurk_test_filled.csv', index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:qa_plausibility] *",
   "language": "python",
   "name": "conda-env-qa_plausibility-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
