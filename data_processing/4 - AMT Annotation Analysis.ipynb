{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) AMT Annotation Analysis\n",
    "Given annotations from Amazon Mechanical Turkers, visualize the results and manually relabel as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from preprocessing.custom_tokenizer import custom_tokenize\n",
    "from error_analysis import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Turker Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_model_predictions(row):\n",
    "    vals = []\n",
    "    for col in ['prediction', 'predicted_answer', 'f1']:\n",
    "        if col in row: \n",
    "            vals.append(\"{}: {}\".format(col, row[col]))\n",
    "    extra_vals = (\", \".join(str(v) for v in vals))\n",
    "    print(\"{} {} [{}, {}]; {}\".format(row['q_tokenization'], row['r_tokenization'], row['answer'], row['label'], extra_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'data/second_MTurk_test_filled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file, index_col='index')\n",
    "df['r_tokenization'] = df.r_tokenization.apply(lambda x : pd.eval(x) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = df.groupby('worker_id')['worker_id'].value_counts()\n",
    "worker_ids = [id[0] for id in counts_df[counts_df > len(df)//10].index.values] # handle weird nesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_turker_responses(df, csv_file, worker_ids=worker_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Summary Per Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_worker_ids = []\n",
    "for worker_id in df.worker_id.dropna().unique():\n",
    "    worker_specific_df = df[df.worker_id == worker_id]\n",
    "    total_gold_labels = len(worker_specific_df[pd.notnull(worker_specific_df.gold_q_relevant)])\n",
    "    if total_gold_labels == 0: continue\n",
    "    num_correct = len(worker_specific_df[(worker_specific_df.q_relevant == worker_specific_df.gold_q_relevant) \n",
    "                                     & (worker_specific_df.r_relevant == worker_specific_df.gold_r_relevant)])\n",
    "    accuracy = 100*num_correct//total_gold_labels\n",
    "    print(f\"Worker ID: {worker_id} ({accuracy}% correct of {total_gold_labels} examined, {len(worker_specific_df)} total)\") \n",
    "    if accuracy < 80:\n",
    "        problematic_worker_ids.append(worker_id)\n",
    "# overall accuracy\n",
    "total_gold_labels = len(df[pd.notnull(df.gold_q_relevant)])\n",
    "num_correct = len(df[(df.q_relevant == df.gold_q_relevant) & (df.r_relevant == df.gold_r_relevant)])\n",
    "accuracy = 100*num_correct//total_gold_labels\n",
    "print(f\"{accuracy}% correct of {total_gold_labels} examined, representing {total_gold_labels*100/len(df)}% of df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Relabel for Workers With Low Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_turker_responses(df, csv_file, worker_ids=problematic_worker_ids, target_num=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
